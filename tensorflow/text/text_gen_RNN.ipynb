{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_gen_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP/3KOVarcwyq7k6SMOv4L6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBQBoIhIHld4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_we-38SdH88X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qi2KjPaIAKW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "46d49492-32a4-4cde-baa1-45a3b055327c"
      },
      "source": [
        "text = open(path_to_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Length of text: {len(text)} characters\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw46Gi2-IKaR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "511fba51-d26e-4b0f-f25f-d954e9d68395"
      },
      "source": [
        "print(text[:250])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOg3ZXVkIOsS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "35f94612-3a61-45ba-fb08-8588f233adda"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f\"{len(vocab)} unique characters\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikigo0ArIY48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8DaBwHrIqMD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "4425147b-cb75-4cc5-fd75-16042d94cf3a"
      },
      "source": [
        "print(\"{\")\n",
        "for char, _ in zip(char2idx, range(20)):\n",
        "  print(f\"  {repr(char):4s}: {char2idx[char]:3d},\")\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQe8KSM6JJVK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "c755aa30-cb45-4dab-b120-82c6835c894c"
      },
      "source": [
        "print(f\"{text[:13]} ---- characters are mapped to int ----> {text_as_int[:13]}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen ---- characters are mapped to int ----> [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1Ruso1HJ5cI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "6dc472d2-d6a2-44a2-96f5-39ff2ea6d05d"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMC8qbeYLSca",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "0d02056f-96ee-417e-edcf-505ca21d9c17"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(\"\".join(idx2char[item.numpy()])))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afmj4l-ILhSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoHs-3WzOxem",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b35f77ea-a057-491b-8249-065fecf2eba3"
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"Input data: \", repr(\"\".join(idx2char[input_example.numpy()])))\n",
        "  print(\"Target data: \", repr(\"\".join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data:  'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANYLXD5aSTqB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "c055c8de-9add-47a3-a461-c3260dc4bc92"
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "  print(f\"Step {i:4d}\")\n",
        "  print(f\"  input: {input_idx} ({repr(idx2char[input_idx])})\")\n",
        "  print(f\"  expected output: {target_idx} ({repr(idx2char[target_idx])})\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i')\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r')\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s')\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t')\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "106pJWaRaR6p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76f5d6ec-97c5-44e0-8fc4-9924ca1fb1b1"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkfzNsJNftdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "embedding_dim = 256\n",
        "\n",
        "rnn_units = 1024"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7s0XE4YhL9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "      tf.keras.layers.GRU(rnn_units,\n",
        "                          return_sequences=True,\n",
        "                          stateful=True,\n",
        "                          recurrent_initializer='glorot_uniform'),\n",
        "      tf.keras.layers.Dense(vocab_size)                         \n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bxNSzM8jdL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-lclXVgj92K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f43272b9-2aaf-4b8d-f204-bddf4843941d"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R37sk22-kSei",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "06850174-47b6-42cf-f8b9-9a58e2f614f3"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHKEhdwQkcJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo6luZDyktJj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "1d99303d-7771-429e-cb08-39853414f13b"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([27, 20, 60, 30, 38, 43,  5, 16, 48, 45,  7, 26,  7, 46, 51, 59, 57,\n",
              "       34, 37,  0, 37, 25,  3, 14, 58, 54, 31,  1, 62, 54, 36, 48, 28, 47,\n",
              "       48, 30, 19, 30,  5,  0, 30, 54, 62, 19, 20, 11,  0, 35, 22, 59, 62,\n",
              "       37, 47, 13, 56, 26, 11, 20, 37, 45, 50, 35, 46, 45, 41, 44,  6,  9,\n",
              "        4, 39,  4, 51, 62, 35, 45, 40, 17, 46,  2, 41,  2, 60, 42, 20, 30,\n",
              "       58, 46, 52, 43, 28,  1, 15,  7, 64, 32, 23, 62, 17, 60, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otjoirMIkuvM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9fc870f9-de5e-4fd1-fd89-089821ecfc57"
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " \"ttle for their dens,\\nPoor harmless lambs abide their enmity.\\nWeep, wretched man, I'll aid thee tear \"\n",
            "\n",
            "Next Char Predictions: \n",
            " \"OHvRZe'Djg-N-hmusVY\\nYM$BtpS xpXjPijRGR'\\nRpxGH;\\nWJuxYiArN;HYglWhgcf,3&a&mxWgbEh!c!vdHRthneP C-zTKxEv:\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCZgurB1lBsT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9bd0bede-efc0-4b17-e02b-15ad8493fefa"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.175061\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUJouKk0llgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss=loss)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVAKUPCYlq5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = \"./training/checkpoints\"\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4Qv6RV3l9bz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=10"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06lJF6uIl_Kh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "82c269fd-3b5a-40af-cc3e-327182055ce8"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 10s 58ms/step - loss: 2.6669\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 10s 59ms/step - loss: 1.9698\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 10s 61ms/step - loss: 1.6982\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 10s 61ms/step - loss: 1.5492\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 10s 61ms/step - loss: 1.4604\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 10s 60ms/step - loss: 1.3997\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 10s 59ms/step - loss: 1.3542\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 10s 59ms/step - loss: 1.3148\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 10s 59ms/step - loss: 1.2803\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 10s 60ms/step - loss: 1.2488\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE_S7aJxmDOd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4834a9c1-7d1a-4734-9a2f-d3daa4d37890"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./training/checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47NCLZ4WmhtK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4JYwac3mw0p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "12c15051-2e49-41a2-fc2b-93c00df6ae6e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLXCY7gemxtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  num_generate = 1000\n",
        "\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = 1.0\n",
        "\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "  \n",
        "  return (start_string + \"\".join(text_generated))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTpOGQczqs1J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "3e146fd2-2baa-45dd-97ae-ddc61a32bb7e"
      },
      "source": [
        "print(generate_text(model, start_string=u\"DILAWAR: \"))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DILAWAR: I cannot Playe.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "What men indeed\n",
            "Whom I poul with us, sights the journey, by mercy,\n",
            "Threefold rudeth tent:\n",
            "'Tis very-liner's child of pale.\n",
            "\n",
            "LADY CARUSBY:\n",
            "It is mightst way the fresh hand.\n",
            "\n",
            "VINCENTIO:\n",
            "It is a rattle, that he may sound the bankles\n",
            "To pleasadians careless rooty Dukes grown chance,\n",
            "By warwick creature to the purpose. We have proved hours for an hour mysceeting betite the law,\n",
            "And wings my trustful conclude hate the dugglests as my liege,\n",
            "Your highness' true enough, then as the Loom of Year;\n",
            "What, grace two your queen, and\n",
            "Spring fortune may be a scarve ear\n",
            "the shepherd would be sins: but lemphy young part both.\n",
            "\n",
            "BENVOLIO:\n",
            "Thou livest friar! who shall fite to me with jounting\n",
            "his imlehicy stalled father forms,\n",
            "Against your son acquaint, never hars, my gracious lady\n",
            "and make war, lend without as he will never and\n",
            "my dame of woes,\n",
            "Ere Dreportance please too trued by a remedy,\n",
            "I shall learn; Sir doom! I have spoken'd\n",
            "With one all her born home will Itrese,\n",
            "Wh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvalJzJIq8bJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYhnBy8pt7il",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjNIv6UIt_UF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inp)\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.keras.losses.sparse_categorical_crossentropy(\n",
        "            target, predictions, from_logits=True\n",
        "        )\n",
        "    )\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br4SLT0N0r31",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4a4c067b-4a15-48ae-faf5-7e7bbe69ed39"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  hidden = model.reset_states()\n",
        "\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    loss = train_step(inp, target)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "      print(f\"Epoch {epoch+1} Batch {batch_n} Loss {loss}\")\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save(checkpoint_prefix.format(epoch=epoch))\n",
        "  \n",
        "  print(f\"Epoch {epoch+1} Loss {loss:.4f}\")\n",
        "  print(f\"Time taken for 1 epoch {time.time() - start}\\n\")\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.173630714416504\n",
            "Epoch 1 Batch 100 Loss 2.3301644325256348\n",
            "Epoch 1 Loss 2.1402\n",
            "Time taken for 1 epoch 11.80069637298584\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.1618611812591553\n",
            "Epoch 2 Batch 100 Loss 1.9473545551300049\n",
            "Epoch 2 Loss 1.7673\n",
            "Time taken for 1 epoch 11.020553827285767\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.7785708904266357\n",
            "Epoch 3 Batch 100 Loss 1.66019606590271\n",
            "Epoch 3 Loss 1.6172\n",
            "Time taken for 1 epoch 10.622749328613281\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.5780038833618164\n",
            "Epoch 4 Batch 100 Loss 1.5075205564498901\n",
            "Epoch 4 Loss 1.4686\n",
            "Time taken for 1 epoch 10.42206358909607\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.4786056280136108\n",
            "Epoch 5 Batch 100 Loss 1.529009222984314\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: ./training/checkpoints/ckpt_4/assets\n",
            "Epoch 5 Loss 1.4432\n",
            "Time taken for 1 epoch 13.950747966766357\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.3466796875\n",
            "Epoch 6 Batch 100 Loss 1.3933459520339966\n",
            "Epoch 6 Loss 1.4007\n",
            "Time taken for 1 epoch 10.221660614013672\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.3236080408096313\n",
            "Epoch 7 Batch 100 Loss 1.322611689567566\n",
            "Epoch 7 Loss 1.3473\n",
            "Time taken for 1 epoch 10.365132331848145\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.3005397319793701\n",
            "Epoch 8 Batch 100 Loss 1.3088958263397217\n",
            "Epoch 8 Loss 1.3119\n",
            "Time taken for 1 epoch 10.556039094924927\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.2023495435714722\n",
            "Epoch 9 Batch 100 Loss 1.3252121210098267\n",
            "Epoch 9 Loss 1.2796\n",
            "Time taken for 1 epoch 10.774481058120728\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.2014623880386353\n",
            "Epoch 10 Batch 100 Loss 1.2269158363342285\n",
            "INFO:tensorflow:Assets written to: ./training/checkpoints/ckpt_9/assets\n",
            "Epoch 10 Loss 1.2434\n",
            "Time taken for 1 epoch 14.088813543319702\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs5FDKgs2b7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fAgtv8e5n1E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "outputId": "577c4d40-ba24-44b5-881a-c365729d8c4d"
      },
      "source": [
        "print(generate_text(model, start_string=u\"DILAWAR: \"))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DILAWAR: I beseech your honsers, word;\n",
            "And so well; he sigh'd my daughter's showing of thy daughter\n",
            "To such necesing\n",
            "To guising and as an oath'd for inWY:\n",
            "For Gloucester, think you, madam:\n",
            "Why, she is, CATESBY:\n",
            "What, comes two bawd:\n",
            "We cannot poor the two sleeping Bianca;\n",
            "And I hide scend of swift,\n",
            "and raves him evermetted these which we, the more as there,\n",
            "Or I am puts in 'Coriolanus: the hair\n",
            "rehol idle, my Lord of Norkold,\n",
            "For Yereon either come to your saints crown'd Tillwife prayers!\n",
            "And say they too much than so't so:\n",
            "For losh being such idlinent and Anoubhes?\n",
            "Cannot be at love, too perpetually, for, good my teother; but I\n",
            "have extreme me surn and Ramelian process;\n",
            "For one belovious dreams,\n",
            "For no done supposemonother's road.\n",
            "\n",
            "Provost:\n",
            "The necks be overto's descent, think it hath done tearing\n",
            "I bear them from thy sinkle thoughts;\n",
            "The death were compass,\n",
            "Corrupt the deed is his true were all my face.\n",
            "D, Lord by the issue of this.\n",
            "Your sins, rebels that did fin me well:\n",
            "The people are flad-\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}