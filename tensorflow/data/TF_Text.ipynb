{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF.Text.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM+3DCrv/uhP/gaC7JIOOpw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBJ2ogPsC-WA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y7jGef3EDx3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "95df9b85-8099-4b73-b181-187ddde7285a"
      },
      "source": [
        "!pip install -q tensorflow-text"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.0MB 2.7MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfux8uoyEMcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_text as text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO7ONymqEP0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs = tf.constant([u'Everything not saved will be lost.'.encode('UTF-16-BE'), u'Sad☹'.encode('UTF-16-BE')])\n",
        "utf8_docs = tf.strings.unicode_transcode(docs, input_encoding='UTF-16-BE', output_encoding='UTF-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCBkxGVwEsWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "fdf615df-2377-4e6f-906b-7ae1b9147b8b"
      },
      "source": [
        "tokenizer = text.WhitespaceTokenizer()\n",
        "tokens = tokenizer.tokenize(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
        "print(tokens.to_list())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:180: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
            "Instructions for updating:\n",
            "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n",
            "[[b'everything', b'not', b'saved', b'will', b'be', b'lost.'], [b'Sad\\xe2\\x98\\xb9']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naMeQOyLFD2z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2fcfab25-2881-4170-d7d3-fd2df01e2f24"
      },
      "source": [
        "tokenizer = text.UnicodeScriptTokenizer()\n",
        "tokens = tokenizer.tokenize(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
        "print(tokens.to_list())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[b'everything', b'not', b'saved', b'will', b'be', b'lost', b'.'], [b'Sad', b'\\xe2\\x98\\xb9']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzo1HL7NFUD6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e82115b7-e65c-4e49-e867-2f9e9fcfec9e"
      },
      "source": [
        "tokens = tf.strings.unicode_split([u\"仅今年前\".encode('UTF-8')], 'UTF-8')\n",
        "print(tokens.to_list())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[b'\\xe4\\xbb\\x85', b'\\xe4\\xbb\\x8a', b'\\xe5\\xb9\\xb4', b'\\xe5\\x89\\x8d']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o-twC-wFgE7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "eb28abef-2dfa-4e46-c830-2f24c5cb1a0a"
      },
      "source": [
        "tokenizer = text.UnicodeScriptTokenizer()\n",
        "(tokens, offset_starts, offset_limits) = tokenizer.tokenize_with_offsets(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
        "print(tokens.to_list())\n",
        "print(offset_starts.to_list())\n",
        "print(offset_limits.to_list())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[b'everything', b'not', b'saved', b'will', b'be', b'lost', b'.'], [b'Sad', b'\\xe2\\x98\\xb9']]\n",
            "[[0, 11, 15, 21, 26, 29, 33], [0, 3]]\n",
            "[[10, 14, 20, 25, 28, 33, 34], [3, 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqOGNkEZGT0i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7b6aeece-5560-4beb-d8a0-0d5f9ef78c56"
      },
      "source": [
        "docs = tf.data.Dataset.from_tensor_slices([['Never tell me the odds.'], [\"It's a trap!\"]])\n",
        "tokenizer = text.WhitespaceTokenizer()\n",
        "tokenized_docs = docs.map(lambda x: tokenizer.tokenize(x))\n",
        "iterator = iter(tokenized_docs)\n",
        "print(next(iterator).to_list())\n",
        "print(next(iterator).to_list())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[b'Never', b'tell', b'me', b'the', b'odds.']]\n",
            "[[b\"It's\", b'a', b'trap!']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAfY6SyaGg2E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4a9e331d-ba5f-47a0-fa93-a4a0cd281506"
      },
      "source": [
        "tokenizer = text.WhitespaceTokenizer()\n",
        "tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
        "\n",
        "f1 = text.wordshape(tokens, text.WordShape.HAS_TITLE_CASE)\n",
        "f2 = text.wordshape(tokens, text.WordShape.IS_UPPERCASE)\n",
        "f3 = text.wordshape(tokens, text.WordShape.HAS_SOME_PUNCT_OR_SYMBOL)\n",
        "f4 = text.wordshape(tokens, text.WordShape.IS_NUMERIC_VALUE)\n",
        "\n",
        "print(f1.to_list())\n",
        "print(f2.to_list())\n",
        "print(f3.to_list())\n",
        "print(f4.to_list())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[True, False, False, False, False, False], [True]]\n",
            "[[False, False, False, False, False, False], [False]]\n",
            "[[False, False, False, False, False, True], [True]]\n",
            "[[False, False, False, False, False, False], [False]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WXqZgALHZJC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bba77347-775c-47c2-bd0c-f4be7a78fbb6"
      },
      "source": [
        "tokenizer = text.WhitespaceTokenizer()\n",
        "tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
        "\n",
        "bigrams = text.ngrams(tokens, 2, reduction_type=text.Reduction.STRING_JOIN)\n",
        "\n",
        "print(bigrams.to_list())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[b'Everything not', b'not saved', b'saved will', b'will be', b'be lost.'], []]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}